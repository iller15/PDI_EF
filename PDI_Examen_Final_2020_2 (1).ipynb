{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peNv8JLDkSHF"
   },
   "source": [
    "# Procesamiento Digital de Imágenes - Examen Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWNjYh93k-Rw"
   },
   "source": [
    "## Instrucciones\n",
    "\n",
    "1.\tEl examen consta de 1 pregunta y tendrá 1 semana para resolverla con su equipo del trabajo final (en caso de sobrar uno o dos alumnos pueden crearse, máximo dos equipos de 2 integrantes).\n",
    "2.\tEl trabajo será entregado, en el aula virtual, hasta las 7:59:59 am del viernes 8 de diciembre. En seguida habrá una exposición, máximo 10 minutos por grupo, de los grupos de trabajo de 8 a 10 am. **BAJO NINGUN MOTIVO SE ACEPTARAN EXAMENES FUERA DEL LIMITE DE TIEMPO INDICADO**\n",
    "3.\tEl examen cuenta con un docente académico, el cual estará conectado durante los primeros 20 minutos del examen.\n",
    "4.\tLas dudas conceptuales sobre el examen han de presentarse dentro de los primeros 20 minutos mediante un correo al profesor GONZALEZ VALENZUELA, RICARDO EUGENIO a pcsirgon@upc.edu.pe.\n",
    "5.\tLos inconvenientes técnicos pueden presentarse a pasado los primeros 20 minutos, puede comunicarlo al profesor GONZALEZ VALENZUELA, RICARDO EUGENIO a pcsirgon@upc.edu.pe.\n",
    "6.\tEl profesor en mención solo recibirá correos provenientes de las cuentas UPC, de ninguna manera se recibirán correos de cuentas públicas. \n",
    "7.\tAnte problemas técnicos, debe de forma obligatoria adjuntar evidencias del mismo, como capturas de pantalla, videos, fotos, etc. Siendo requisito fundamental que, en cada evidencia se pueda apreciar claramente la fecha y hora del sistema operativo del computador donde el alumno está rindiendo el examen. \n",
    "8.\tLos correos sobre problemas técnicos se recibirán hasta 15 minutos luego de culminado el examen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77aKbY1YFl3g"
   },
   "source": [
    "## Integrantes\n",
    "\n",
    "*   Alumno 1: <font color='green'> u20181a010 - Joaquin Adrian Galvan Diaz</font><br>\n",
    "*   Alumno 2: <font color='green'> u201818067 - Dante Brandon Moreno Carhuacusma</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLrugUgnlIMN"
   },
   "source": [
    "## Caso de Estudio - Video Summarization\n",
    "\n",
    "1. Descargar y/o crear videos de 10 a 15 minutos y efectuar un resumen de tiempo en los mismos.\n",
    "\n",
    "2. Aplicando únicamente técnicas de procesamiento digital de imágenes, segmentar los objetos que aparecen en diferentes instantes y sobreponerlos en una cantidad muy inferior de frames etiquetando cada objeto con los insantes de tiempo en que aparece en el video.\n",
    "\n",
    "3. En el siguiente [video](https://www.youtube.com/watch?v=gk3qTMlcadk), podrá tener un mejor concepto de lo solicitado. \n",
    "\n",
    "4.\tDocumente sus métodos y elecciones. Explique su metodología. Codifique su solución. Obtenga resultados y realice comparaciones. Redacte sus conclusiones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIvisRJOJgJ_"
   },
   "source": [
    "## Resolución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r02P9REKJkS1"
   },
   "source": [
    "### Metodología (6 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYL90_sGsXQv"
   },
   "source": [
    "<font color='green'>Aquí **enumere** y **explique** los pasos de su metodología </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer método: Diferencia de imágenes y ROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro primer método, asumimos que el primer frame del video está \"limpio\", es decir, es la base sobre la cual detectar los nuevos objetos en la imagen. Definimos entonces:\n",
    "\n",
    " * $A$: Primer frame del video.\n",
    " * $B_{n}$: n-ésimo frame del video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera tarea es identificar regiones de interés (ROIs) donde ocurran cambios en el video. Luego se podrá utilizar esta información para calcular juntar los objetos en el resumen. El algoritmo de es:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Por cada $B_{n}$:\n",
    "> $ROIs$: Arreglo que guardará las regiones de interés en el frame.\n",
    "> - 1.1. Hallar la diferencia: $D_{n}$ = $A$ - $B_{n}$. **Si** $B_{n}$ es similar a $A$, $D_{n}$ estará compuesto en su mayoría por $0s$.\n",
    "> - 1.2. Aplicar filtro de la media o mediana sobre $D_{n}$, ya que puede ser afectado por el rudio\n",
    "> - 1.3. Binarizar o Aplicar un operador clásico de Edge Detection sobre $D_{n}$\n",
    "> - 1.4. $contornos \\leftarrow findContours(D_{n})$\n",
    "> - 1.5. Por cada $c$ en $contornos$:\n",
    ">> * Agregar rectángulo envolvente de $c$ al arreglo $ROIs$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El arreglo $ROIs$ guarda las regiones de interés en un frame detectadas. Para la selección, queremos guardar un identificador del objeto para los diferentes frames. Además, se debe guardar el momento en el tiempo en el que aparece una región de interés. Entonces, definimos \"Objeto\" en el video, donde cada \"Objeto\" aparece en distintas ROIs en cada frame del video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvNvTKIFJvwQ"
   },
   "source": [
    "### Implementación (6 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pafy\n",
      "  Downloading pafy-0.5.5-py2.py3-none-any.whl (35 kB)\n",
      "Installing collected packages: pafy\n",
      "Successfully installed pafy-0.5.5\n",
      "Collecting youtube-dl\n",
      "  Downloading youtube_dl-2020.12.2-py2.py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: youtube-dl\n",
      "Successfully installed youtube-dl-2020.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pafy\n",
    "!pip install youtube-dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LQoyq0_GJ5aB"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pafy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar el vídeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_imshow(imagen, titulo = \"Imagen\"):\n",
    "    cv2.imshow(titulo,imagen)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=CkVJyAKwByw\"\n",
    "video = pafy.new(url)\n",
    "best = video.getbest(preftype=\"mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducir el video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Frame_Prueba_0 = None\n",
    "Frame_Prueba_X = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture()\n",
    "capture.open(best.url)\n",
    "\n",
    "j = 0\n",
    "\n",
    "anterior = None\n",
    "start = True\n",
    "\n",
    "while (capture.isOpened()):\n",
    "    ##Por cada frame del video:\n",
    "    #capturar y mostrar el frame\n",
    "    ret, frame = capture.read()\n",
    "    #cv2.imshow(\"It's meta\",frame)\n",
    "    frame = cv2.cvtColor(frame,cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    if j == 0:\n",
    "        Frame_Prueba_0 = frame\n",
    "    elif j == 180:\n",
    "        Frame_Prueba_X = frame\n",
    "    \n",
    "    if not start:        \n",
    "        dif = anterior - frame\n",
    "        dif = cv2.adaptiveThreshold(frame,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,0)\n",
    "        cv2.imshow(\"Diferencia\",dif)\n",
    "    else:\n",
    "        start = False    \n",
    "    anterior = frame\n",
    "    \n",
    "    j += 1\n",
    "    \n",
    "    if(cv2.waitKey(20) & 0xFF == ord('q')):\n",
    "        anterior = dif\n",
    "        break\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cv2.imshow(\"Diferencia\",anterior)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas Dante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficarRectDeROIs(imagen, ROIs):\n",
    "    for roi in ROIs:\n",
    "        (x,y,w,h) = roi\n",
    "        cv2.rectangle(imagen, (x,y), (x+w,y+h), (255,0,0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturar los primeros \"N\" frames para poder testear.\n",
    "Se baja la resolución para evitar carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Prueba = 240 #Cantidad de frames de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480 853\n"
     ]
    }
   ],
   "source": [
    "nH = int(anterior.shape[0] // 1.5)\n",
    "nW = int(anterior.shape[1] // 1.5)\n",
    "print(nH,nW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones adicionales para ayudar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areaInterseccion(roi1, roi2):\n",
    "    def dEje(tuplaS): #tuplaS: [ (x1, x1 + w1) , (x2, x2 + w2) ]\n",
    "        def auxResta(v1, v2): #.... Devuelve la resta, si es negativo, devuelve 0\n",
    "            d = v1 - v2\n",
    "            if d < 0:\n",
    "                return 0\n",
    "            return d\n",
    "        im = 0 #id del segmento más cerca a 0\n",
    "        iM = 1 #id del segmento más lejos de 0\n",
    "        if tuplaS[1][0] < tuplaS[0][0]:\n",
    "            im = 1\n",
    "            iM = 0\n",
    "        return (tuplaS[im][1] - tuplaS[iM][0]) - auxResta(tuplaS[im][1], tuplaS[iM][1])\n",
    "    (x1, y1, w1, h1) = roi1\n",
    "    (x2, y2, w2, h2) = roi2\n",
    "    if x1 < x2 + w2 and x1 + w1 > x2 and y1 < y2 + h2 and y1 + h1 > y2: # Si colisionan\n",
    "        dx = dEje([(x1, x1+w1), (x2, x2+w2)])\n",
    "        dy = dEje([(y1, y1+h1), (y2, y2+h2)])\n",
    "        return dx * dy\n",
    "    return 0\n",
    "def roiUroi(roi1, roi2):\n",
    "    nXmin = min(roi1[0],roi2[0])\n",
    "    nXmax = max(roi1[0] + roi1[2],roi2[0] + roi2[2])\n",
    "    nYmin = min(roi1[1],roi2[1])\n",
    "    nYmax = max(roi1[1] + roi1[3],roi2[1] + roi2[3])\n",
    "    return (nXmin, nYmin, nXmax - nXmin, nYmax - nYmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colisionan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areaInterseccion((0,0,3,3),(0,0,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## =========================== LEER ========================\n",
    "# Se está agregando un limitador para no tener \"rois\" muy pequeñas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna un arreglos de ROIs de un frame n, comparando con el frame 0\n",
    "def roisDeFrame(f0, fn, dimKernelBlur, umbralDif): # BTW: dimKernelBlur debe ser impar\n",
    "    if len(f0.shape) > 2:\n",
    "        f0 = cv2.cvtColor(f0,cv2.COLOR_RGB2GRAY)\n",
    "    if len(fn.shape) > 2:\n",
    "        fn = cv2.cvtColor(fn,cv2.COLOR_RGB2GRAY)\n",
    "    f0 = np.array(f0, dtype=np.int16) #..... Permitir los valores negativos en la resta inicial :)\n",
    "    fn = np.array(fn, dtype=np.int16) #..... \"\"\n",
    "    D = np.array(np.abs(f0 - fn), dtype=np.uint8)\n",
    "    D = cv2.medianBlur(D,dimKernelBlur)\n",
    "    _, D = cv2.threshold(D,umbralDif,255,cv2.THRESH_BINARY)\n",
    "    contornos, _ = cv2.findContours(D,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    ROIs = []\n",
    "    for contorno in contornos:\n",
    "        x,y,w,h = cv2.boundingRect(contorno)\n",
    "        ROIs.append((x,y,w,h))\n",
    "    return ROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas con dos frames del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2_imshow(Frame_Prueba_0)\n",
    "cv2_imshow(Frame_Prueba_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "auxRois = roisDeFrame(Frame_Prueba_0, Frame_Prueba_X,13,110)\n",
    "print(len(auxRois))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajamos con un DataFrame de Objetos:\n",
    ">ID | ROI | Frame | Segundo\n",
    ">--- | --- | --- | ---\n",
    ">0 | (x, y, w, h) | $f_{0}$ | $t_{0}$\n",
    ">0 | (x, y, w, h) | $f_{1}$ | $t_{0}$\n",
    ">0 | (x, y, w, h) | $f_{2}$ | $t_{1}$\n",
    ">1 | (x, y, w, h) | $f_{1}$ | $t_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asignarObjetos(objetos, rois_t, t_frame, mseg, umbralRatioAr):\n",
    "    # Comparamos los rois en un frame 't'con los rois asignados a objetos en el frame previo (cuando lo llaman debe validar eso)\n",
    "    # (Asume que debe haber continuidad, si un objeto sale de la pantalla y regresa, lo toma como otro)\n",
    "    # Utiliza un umbral de intersección para decidir si dos rois son del mismo objeto en diferentes frames\n",
    "    # RETORNA: arreglo de filas a agregar al DataFrame, con su ID de objeto, ROI, frame y segundo\n",
    "    agregados = []\n",
    "    for i in range(len(objetos)): #........ Por cada uno de los objetos en el frame t\n",
    "        objeto = objetos.iloc[i]\n",
    "        roi = objeto[\"ROI\"] #................. Compararemos su roi\n",
    "        for roi_t in rois_t: #................ con cada roi detectado en el nuevo frame\n",
    "            areaI = areaInterseccion(roi, roi_t) \n",
    "            if areaI / (roi[2] * roi[3]) > umbralRatioAr and areaI / (roi_t[2] * roi_t[3]) > umbralRatioAr: # Si el área de la intersección supera el umbral en ambos\n",
    "                agregados.append([objeto[\"ID\"], roi_t, t_frame, mseg]) #.... Agrega a un arreglo de \"filas a agregar\" al DataFrame objetos\n",
    "                rois_t.remove(roi_t) #...................................... Ya no tiene que analizar roi_t porque ya se le asignó id objeto\n",
    "    # -------- Pueden haber sobrado rois en rois_t, así que asumimos son nuevos objetos ---------------\n",
    "    n_ID = 0\n",
    "    if len(objetos) > 0:\n",
    "        n_ID = max(objetos[\"ID\"]) + 1 #.................. Empezar a enumerar a los objetos desde n_ID\n",
    "    for roi_t in rois_t:\n",
    "        agregados.append([n_ID, roi_t, t_frame, mseg])\n",
    "        n_ID += 1\n",
    "    return agregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiarDF(dfObjetos):\n",
    "    idsObjetos = pd.unique(dfObjetos[\"ID\"])\n",
    "    for i in idsObjetos:\n",
    "        aux = dfObjetos[dfObjetos[\"ID\"] == i]\n",
    "        if len(aux) < 30 and len(aux) > 0:\n",
    "            aux = dfObjetos.index[dfObjetos[\"ID\"] == i].tolist()\n",
    "            dfObjetos = objetos.drop(aux)\n",
    "    return dfObjetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realización:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ROI</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Segundo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(248, 220, 7, 6)</td>\n",
       "      <td>161</td>\n",
       "      <td>5.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID               ROI Frame  Segundo\n",
       "0  0  (248, 220, 7, 6)   161     5.41"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \" *Vxp \" significa \"Experimentar con los valores\"\n",
    "capture.open(best.url)\n",
    "_, A = capture.read() #.......... Frame \"Original\", se asume que en este no hay \"objetos\" en pantalla\n",
    "A = cv2.cvtColor(A,cv2.COLOR_RGB2GRAY)\n",
    "nH = int(A.shape[0] // 1.5)\n",
    "nW = int(A.shape[1] // 1.5)\n",
    "objetos = [] #................... Arreglo que se espera sea: [ [ [(x,y,w,h), frame, milisegundo], \n",
    "#................................                                [(x,y,w,h), frame, milisegundo],...] , [ [roi, frame, t], ... ]\n",
    "A = cv2.resize(A, (nW,nH)) #................ Baja la resolución del frame\n",
    "for i in range(N_Prueba): #................. No ocupa todo el video, por ahora solo frames de prueba\n",
    "    #print(\"============== FRAME %s ==============\"%i)\n",
    "    _, b = capture.read()\n",
    "    ms = capture.get(0) # Get los milisegundos\n",
    "    ms = round( ms/1000 ,2)\n",
    "    b = cv2.cvtColor(b,cv2.COLOR_RGB2GRAY)\n",
    "    b = cv2.resize(b, (nW,nH))\n",
    "    rois = roisDeFrame(A, b, 15, 130) # * Vxp, la dimensión del kernel debe ser impar\n",
    "    #print(\">> len(objetos): \", len(objetos))\n",
    "    #print(\">> ROIs crudos: \",len(rois))\n",
    "    #---- Pasar solo los objetos que aparecieron en el frame (i-1):\n",
    "    objetos_previos = []\n",
    "    if i > 0 and len(objetos) > 0:\n",
    "        objetos_previos = objetos[objetos[\"Frame\"] == i - 1]\n",
    "    nuevasFilas = asignarObjetos(objetos_previos,rois,i,ms,0.6) # * Vxp, umbral de ratio de area interseccion\n",
    "    #---- nuevasFilas está en el formato, pero \"objetos\" puede aún ser nulo, así que valida:\n",
    "    nuevasFilas = pd.DataFrame(data=nuevasFilas,columns=[\"ID\",\"ROI\",\"Frame\",\"Segundo\"])\n",
    "    if len(objetos) > 0:\n",
    "        objetos = objetos.append(nuevasFilas,ignore_index=True) # ... Si ya existen objetos, añade los nuevos\n",
    "    else:\n",
    "        objetos = nuevasFilas #...................................... Si nunca hubieron, ahora hay\n",
    "capture.release()\n",
    "print(len(pd.unique(objetos[\"ID\"])))\n",
    "objetos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "objetos = limpiarDF(objetos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15825"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(objetos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficarRectDeObjetos(imagen, objetos):\n",
    "    for i in range(len(objetos)):\n",
    "        objeto = objetos.iloc[i]\n",
    "        (x,y,w,h) = objeto[\"ROI\"]\n",
    "        cv2.rectangle(imagen, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "        texto = \"Obj %s en %s ms\"%(objeto[\"ID\"],objeto[\"Segundo\"])\n",
    "        imagen = cv2.putText(imagen, texto, (x,y), cv2.FONT_HERSHEY_SIMPLEX,  \n",
    "                   0.3, (180,255,0), 1, cv2.LINE_AA)\n",
    "    return imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "aux = None\n",
    "capture.open(best.url)\n",
    "while(True):\n",
    "    _, frame = capture.read()\n",
    "    frame = cv2.resize(frame, (nW,nH)) #.... TODO: pasar ROI a la resolución original :c\n",
    "    #arrROIs = objetos[objetos[\"Frame\"] == j][\"ROI\"].values\n",
    "    arrObjs = objetos[objetos[\"Frame\"] == j]\n",
    "    if len(arrObjs) > 0:\n",
    "        frame = graficarRectDeObjetos(frame, arrObjs)\n",
    "    cv2.imshow(\"Diferencia\",frame)\n",
    "    if j == 2:\n",
    "        aux = frame\n",
    "    j += 1\n",
    "    if(cv2.waitKey(20) & 0xFF == ord('q')):\n",
    "        break\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2_imshow(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Material adicional:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OpenCV Python Tutorials. *Getting started with videos*. Recuperado de: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
    "* OpenCV. *Reading and writing images and video* Recuperado de: https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH7A73FhJ51o"
   },
   "source": [
    "### Resultados y Discusión (6 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWS0I-GXJ57i"
   },
   "source": [
    "<font color='green'> Discuta los **varios resultados obtenidos** por la selección de **diversos parámetros seleccionados** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera parte de nuestro método fue la detección de regiones de interés en dos frames distintos. Extraímos los contornos de la imagen de la diferencia de los dos frames. Dentro de este método, aplicamos el filtro de la mediana para disminuir el posible ruido en la imagen $D$ (Diferencia), y binarizamos a partir de un umbral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar los mejores valores en esta sección, tomamos dos frames del video:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame 0: ![Frame 0](Informe_Recursos/Auto0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame X: ![Frame X](Informe_Recursos/AutoX.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, aplicamos nuestro método para detectar las ROIs del frame $X$. Esperamos tener dos regiones de interés: la mujer saliendo del carro y el auto en movimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ayudarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(esperado, arrObtenidos):\n",
    "    arrError = []\n",
    "    for obtenido in arrObtenidos:\n",
    "        arrError.append(abs(esperado - obtenido))\n",
    "    return arrError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEyQ2THGJ6BA"
   },
   "source": [
    "### Conclusiones (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNOKEOoRJ6Ep"
   },
   "source": [
    "<font color='green'> Redacte, al menos, **5 conclusiones relevantes** referentes a como cubrió su objetivo y que le ayudó a optimizar sus resutlados. </font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PDI - Examen Final - 2020-2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
